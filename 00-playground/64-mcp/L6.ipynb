{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc712d2-8b62-4098-a986-76ec60c795c4",
   "metadata": {},
   "source": [
    "# Lesson 6: Connecting the MCP Chatbot to Reference Servers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c374c1-4691-4aed-a145-4b46167ff197",
   "metadata": {},
   "source": [
    "You'll now extend the MCP chatbot capabilities by making it connect to any MCP server. You will integrate the tools of two official MCP servers in addition to the tools of the research server you built in a previous lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2fd0f0-ffdf-4124-ad5a-d734403eb020",
   "metadata": {},
   "source": [
    "<img src=\"images/lesson_6.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216c450-075a-426f-907f-e0fff64df6c5",
   "metadata": {},
   "source": [
    "## Open-Source MCP Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495f41b-301b-4243-b5ab-724407bc338a",
   "metadata": {},
   "source": [
    "In this [repo](https://github.com/modelcontextprotocol/servers), you can find a collection of reference implementations for the MCP servers, as well as references to community built servers and additional resources. You will use two of the reference servers to integrate their tools in your MCP chatbot:\n",
    "- [fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch): provides the `fetch` tool which fetches a URL from the internet and extracts its contents as markdown.\n",
    "- [filesystem](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem): provides several tools for interacting with the files and directories within a directory that you specify.\n",
    "\n",
    "You can check the readme file of each server to check the features they expose and how to run them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f68ad1-d038-4e31-809e-3704a4433d1b",
   "metadata": {},
   "source": [
    "## Updating the MCP Chatbot - Optional Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f25f1-5ee2-4cff-b1b1-fc5db7145a62",
   "metadata": {},
   "source": [
    "Here are the updates you'll make to the chatbot. You're encouraged to read this section before or after you watch the video, if you'd like to learn more about the details of the code.\n",
    "\n",
    "- Instead of hardcoding the server parameters in the chatbot, the chatbot will read the server configurations from a JSON file:\n",
    "  ### Server Configuration\n",
    "  In the `L6/mcp_project`, you can find the `server_config.json` configuration file that has the following structure.\n",
    "    ``` json\n",
    "    {\n",
    "        \"mcpServers\": {\n",
    "            \n",
    "            \"filesystem\": {\n",
    "                \"command\": \"npx\",\n",
    "                \"args\": [\n",
    "                    \"-y\",\n",
    "                    \"@modelcontextprotocol/server-filesystem\",\n",
    "                    \".\"\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            \"research\": {\n",
    "                \"command\": \"uv\",\n",
    "                \"args\": [\"run\", \"research_server.py\"]\n",
    "            },\n",
    "            \n",
    "            \"fetch\": {\n",
    "                \"command\": \"uvx\",\n",
    "                \"args\": [\"mcp-server-fetch\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "    For the reference servers, the commands `npx` and `uvx` directly install the files of the servers to your local environment (you don't need to install them ahead of time). Note for the `filesystem`, the `.` is provided as the third argument and it means \"current directory\". This means that you're allowing for the `fetch` server to interact with the files and directories that are within the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed070ac-e376-4403-ad76-e91a9011f5a6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b> To Access the  <code>mcp_project</code> folder :</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em> and finally 3) click on <em>L6</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d38af9-7777-4f9e-b314-e882e2b7ca8c",
   "metadata": {},
   "source": [
    "- Here's a rough sketch of the diagram of the updated MCP_chatbot:\n",
    "  \n",
    "  <img src=\"images/updated_class.png\" width=\"600\">\n",
    "\n",
    "  1. Instead of having one session, you now have a list of client sessions where each client session establishes a 1-to-1 connection to each server;\n",
    "  2. `available_tools` includes the definitions of all the tools exposed by all servers that the chatbot can connect to.\n",
    "  3. `tool_to_session` maps the tool name to the corresponding client session; in this way, when the LLM decides on a particular tool name, you can map it to the correct client session so you can use that session to send `tool_call` request to the right MCP server.\n",
    "  4. `exit_stack` is a context manager that will manage the mcp client objects and their sessions and ensures that they are properly closed. In lesson 5, you did not use it because you used the `with` statement which behind the scenes uses a context manager. Here you could again use the `with` statement, but you may end up using multiple nested `with` statements since you have multiple servers to connect to. `exit_stack` allows you to dynamically add the mcp clients and their sessions as you'll see in the code below.\n",
    "  5. `connect_to_servers` reads the server configuration file and for each single server, it calls the helper method `connect_to_server`. In this latter method, an MCP client is created and used to launch the server as a sub-process and then a client session is created to connect to the server and get a description of the list of the tools provided by the server.\n",
    "  6. `cleanup` is a helper method that ensures all your connections are properly shut down when you're done with them. In lesson 5, you relied on the `with` statement to automatically clean up resources. This cleanup method serves a similar purpose, but for all the resources you've added to your exit_stack; it closes (your MCP clients and sessions) in the reverse order they were added - like stacking and unstacking plates. This is particularly important in network programming to avoid resource leaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7159b5-02ba-413a-b30c-4f7725379c46",
   "metadata": {},
   "source": [
    "## Updated Code for the MCP Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee81457-ab55-4acb-9dc6-6b4443344d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/mcp_chatbot.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.sessions: List[ClientSession] = [] # new\n",
    "        self.exit_stack = AsyncExitStack() # new\n",
    "        self.anthropic = Anthropic()\n",
    "        self.available_tools: List[ToolDefinition] = [] # new\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {} # new\n",
    "\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"Connect to a single MCP server.\"\"\"\n",
    "        try:\n",
    "            server_params = StdioServerParameters(**server_config)\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            ) # new\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            ) # new\n",
    "            await session.initialize()\n",
    "            self.sessions.append(session)\n",
    "            \n",
    "            # List available tools for this session\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in tools])\n",
    "            \n",
    "            for tool in tools: # new\n",
    "                self.tool_to_session[tool.name] = session\n",
    "                self.available_tools.append({\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self): # new\n",
    "        \"\"\"Connect to all configured MCP servers.\"\"\"\n",
    "        try:\n",
    "            with open(\"server_config.json\", \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            servers = data.get(\"mcpServers\", {})\n",
    "            \n",
    "            for server_name, server_config in servers.items():\n",
    "                await self.connect_to_server(server_name, server_config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading server configuration: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role':'user', 'content':query}]\n",
    "        response = self.anthropic.messages.create(max_tokens = 2024,\n",
    "                                      model = 'claude-3-7-sonnet-20250219', \n",
    "                                      tools = self.available_tools,\n",
    "                                      messages = messages)\n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            assistant_content = []\n",
    "            for content in response.content:\n",
    "                if content.type =='text':\n",
    "                    print(content.text)\n",
    "                    assistant_content.append(content)\n",
    "                    if(len(response.content) == 1):\n",
    "                        process_query= False\n",
    "                elif content.type == 'tool_use':\n",
    "                    assistant_content.append(content)\n",
    "                    messages.append({'role':'assistant', 'content':assistant_content})\n",
    "                    tool_id = content.id\n",
    "                    tool_args = content.input\n",
    "                    tool_name = content.name\n",
    "                    \n",
    "    \n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                    \n",
    "                    # Call a tool\n",
    "                    session = self.tool_to_session[tool_name] # new\n",
    "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append({\"role\": \"user\", \n",
    "                                      \"content\": [\n",
    "                                          {\n",
    "                                              \"type\": \"tool_result\",\n",
    "                                              \"tool_use_id\":tool_id,\n",
    "                                              \"content\": result.content\n",
    "                                          }\n",
    "                                      ]\n",
    "                                    })\n",
    "                    response = self.anthropic.messages.create(max_tokens = 2024,\n",
    "                                      model = 'claude-3-7-sonnet-20250219', \n",
    "                                      tools = self.available_tools,\n",
    "                                      messages = messages) \n",
    "                    \n",
    "                    if(len(response.content) == 1 and response.content[0].type == \"text\"):\n",
    "                        print(response.content[0].text)\n",
    "                        process_query= False\n",
    "\n",
    "    \n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                    \n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def cleanup(self): # new\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        # the mcp clients and sessions are not initialized using \"with\"\n",
    "        # like in the previous lesson\n",
    "        # so the cleanup should be manually handled\n",
    "        await chatbot.connect_to_servers() # new! \n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup() #new! \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f221f83-6e8b-431e-bd27-6dccedfac408",
   "metadata": {},
   "source": [
    "## Running the MCP Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c35e59-61ba-4bc5-a13f-3022a4e0fced",
   "metadata": {},
   "source": [
    "**Terminal Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04bf05b-10ce-4576-9717-6870a4cfd701",
   "metadata": {},
   "source": [
    "- To open the terminal, run the cell below.\n",
    "- Navigate to the `mcp_project` directory:\n",
    "    - `cd L6/mcp_project`\n",
    "- Activate the virtual environment:\n",
    "    - `source .venv/bin/activate`\n",
    "- Run the chatbot:\n",
    "    - `uv run mcp_chatbot.py`\n",
    "- To exit the chatbot, type `quit`.\n",
    "- If you run some queries and would like to access the `papers` folder or any output files: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L6` -> `mcp_project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afe864e-e394-4366-9497-ae115d40c0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"768\"\n",
       "            src=\"https://s172-29-8-172p8888.lab-aws-production.deeplearning.ai/terminals/3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb4081a24d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new terminal\n",
    "import os\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(f\"{os.environ.get('DLAI_LOCAL_URL').format(port=8888)}terminals/3\", width=600, height=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21edb4-aba4-4b5a-b3c6-8aa908a7c8df",
   "metadata": {},
   "source": [
    "Make sure to interact with the chatbot. Here are some query examples:\n",
    "- Fetch the content of this website: https://modelcontextprotocol.io/docs/concepts/architecture and save the content in the file \"mcp_summary.md\", create a visual diagram that summarizes the content of \"mcp_summary.md\" and save it in a text file\n",
    "- Fetch deeplearning.ai and find an interesting term. Search for 2 papers around the term and then summarize your findings and write them to a file called results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289248c2-51f6-4ccf-ab7e-3c645631f88d",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3cce2-111c-4338-b117-fcc5aeece8a3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> â¬‡ &nbsp; <b>To Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> ðŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4f772-91a7-487b-a52c-ebceb1397434",
   "metadata": {},
   "source": [
    "## Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6a5889-4aea-406f-96eb-8d318d54a01c",
   "metadata": {},
   "source": [
    "You are encouraged to refactor the code of `MCP_ChatBot` if you'd like:\n",
    "\n",
    "- how to connect to servers asynchronously\n",
    "- what if tools from different servers have the same name\n",
    "- revisit the attributes\n",
    "  \n",
    "And maybe any other idea you may think of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff6e3f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
